---
layout: paper
paper-type: inproceedings
authors: Michael Auli and Jianfeng Gao
title: Decoder Integration and Expected BLEU Training for Recurrent Neural Network Language Models
doc-url: papers/acl2014_expbleu_rnn.pdf
booktitle: Proceedings of ACL 
booktitle-url: http://www.cs.jhu.edu/ACL2014
abstract: >
  Neural network language models are often trained by optimizing likelihood, but 
  we would prefer to optimize for a task specific metric, such as BLEU for machine 
  translation.
  We show how a recurrent neural network language model can be optimized towards 
  an expected BLEU loss instead of the usual cross-entropy criterion.
  Furthermore, we tackle the issue of directly integrating a recurrent  
  network into first-pass decoding under an efficient approximation.
  Our best results improve a phrase-based statistical machine translation 
  system trained on WMT 2012 French-English data by up to 2.0 BLEU, and the 
  expected BLEU objective improves over a cross-entropy trained model by up to 
  0.6 BLEU in a single reference setup.
---