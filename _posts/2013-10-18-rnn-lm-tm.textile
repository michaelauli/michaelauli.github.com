---
layout: paper
paper-type: inproceedings
authors: Michael Auli and Michel Galley and Chris Quirk and Geoffrey Zweig
title: Joint Language and Translation Modeling with Recurrent Neural Networks
doc-url: papers/emnlp2013_joint_lm_tm.pdf
booktitle: Proceedings of EMNLP
booktitle-url: http://hum.csse.unimelb.edu.au/emnlp2013/â€Ž
abstract: >
  We present a joint language and transla- tion model based on a 
  recurrent neural network which predicts target words based on 
  an unbounded history of both source and target words. The weaker 
  independence asssumptions of this model result in a vastly larger 
  search space compared to related feed- forward-based language or 
  translation models. We tackle this issue with a new lattice rescoring 
  algorithm and demonstrate its effectiveness empirically. Our joint 
  model builds on a well known recurrent neural network language model 
  (Mikolov, 2012) augmented by a layer of additional inputs from the 
  source language. We show competitive accuracy compared to the 
  traditional channel model features. Our best results improve the output 
  of a system trained on WMT 2012 French-English data by up to 1.5 BLEU, 
  and by 1.1 BLEU on average across several test sets.
---

